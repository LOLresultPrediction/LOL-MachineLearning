{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.metrics import accuracy_score # 정확도 함수\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chanllenger :  result\n",
      " 1    4836\n",
      "-1    4836\n",
      "Name: count, dtype: int64\n",
      "Grandmaster :  result\n",
      " 1    6365\n",
      "-1    6365\n",
      "Name: count, dtype: int64\n",
      "Master :  result\n",
      " 1    8838\n",
      "-1    8838\n",
      "Name: count, dtype: int64\n",
      "Emerald :  result\n",
      " 1    18621\n",
      "-1    18621\n",
      "Name: count, dtype: int64\n",
      "Platinum :  result\n",
      " 1    19067\n",
      "-1    19067\n",
      "Name: count, dtype: int64\n",
      "Gold :  result\n",
      " 1    11332\n",
      "-1    11332\n",
      "Name: count, dtype: int64\n",
      "Silver :  result\n",
      " 1    13167\n",
      "-1    13167\n",
      "Name: count, dtype: int64\n",
      "Bronze :  result\n",
      " 1    19057\n",
      "-1    19057\n",
      "Name: count, dtype: int64\n",
      "Iron :  result\n",
      " 1    18524\n",
      "-1    18524\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "win_10_Chanllenger = pd.read_csv('Dataset/win_10/10_Chanllenger.csv')\n",
    "lose_10_Chanllenger = pd.read_csv('Dataset/lose_10/10_Chanllenger_lose.csv')\n",
    "win_10_Chanllenger_ver2 = pd.read_csv('Dataset/win_10/10_Chanllenger_ver2.csv')\n",
    "lose_10_Chanllenger_ver2 = pd.read_csv('Dataset/lose_10/10_Chanllenger_ver2_lose.csv')\n",
    "Chanllenger1 = pd.concat([win_10_Chanllenger, lose_10_Chanllenger], ignore_index=True)\n",
    "Chanllenger2 = pd.concat([win_10_Chanllenger_ver2, lose_10_Chanllenger_ver2], ignore_index=True)\n",
    "Chanllenger = pd.concat([Chanllenger1, Chanllenger2], ignore_index=True)\n",
    "print('Chanllenger : ', Chanllenger['result'].value_counts())\n",
    "\n",
    "win_10_Grandmaster = pd.read_csv('Dataset/win_10/10_Grandmaster.csv')\n",
    "lose_10_Grandmaster = pd.read_csv('Dataset/lose_10/10_Grandmaster.csv')\n",
    "Grandmaster = pd.concat([win_10_Grandmaster, lose_10_Grandmaster], ignore_index=True)\n",
    "print('Grandmaster : ', Grandmaster['result'].value_counts())\n",
    "\n",
    "# win_Chanllenger = pd.read_csv('Dataset/win/Chanllenger.csv')\n",
    "# lose_Chanllenger = pd.read_csv('Dataset/lose/Chanllenger_lose.csv')\n",
    "# Chanllenger = pd.concat([win_Chanllenger, lose_Chanllenger], ignore_index=True)\n",
    "# print(Chanllenger['result'].value_counts())\n",
    "\n",
    "# win_Grandmaster = pd.read_csv('Dataset/win/Grandmaster.csv')\n",
    "# lose_Grandmaster = pd.read_csv('Dataset/lose/Grandmaster_lose.csv')\n",
    "# Grandmaster = pd.concat([win_Grandmaster, lose_Grandmaster], ignore_index=True)\n",
    "\n",
    "win_Master = pd.read_csv('Dataset/win/Master.csv')\n",
    "lose_Master = pd.read_csv('Dataset/lose/Master_lose.csv')\n",
    "Master = pd.concat([win_Master, lose_Master], ignore_index=True)\n",
    "print('Master : ', Master['result'].value_counts())\n",
    "\n",
    "win_Emerald_I = pd.read_csv('Dataset/win/Emerald_I.csv')\n",
    "lose_Emerald_I = pd.read_csv('Dataset/lose/Emerald_I_lose.csv')\n",
    "win_Emerald_II = pd.read_csv('Dataset/win/Emerald_II.csv')\n",
    "lose_Emerald_II = pd.read_csv('Dataset/lose/Emerald_II_lose.csv')\n",
    "win_Emerald_III = pd.read_csv('Dataset/win/Emerald_III.csv')\n",
    "lose_Emerald_III = pd.read_csv('Dataset/lose/Emerald_III_lose.csv')\n",
    "win_Emerald_IV = pd.read_csv('Dataset/win/Emerald_IV.csv')\n",
    "lose_Emerald_IV = pd.read_csv('Dataset/lose/Emerald_IV_lose.csv')\n",
    "Emerald = pd.concat([win_Emerald_I, win_Emerald_II, win_Emerald_III, win_Emerald_IV, lose_Emerald_I, lose_Emerald_II, lose_Emerald_III, lose_Emerald_IV], ignore_index=True)\n",
    "print('Emerald : ', Emerald['result'].value_counts())\n",
    "\n",
    "win_Platinum_I = pd.read_csv('Dataset/win/Platinum_I.csv')\n",
    "lose_Platinum_I = pd.read_csv('Dataset/lose/Platinum_I_lose.csv')\n",
    "win_Platinum_II = pd.read_csv('Dataset/win/Platinum_II.csv')\n",
    "lose_Platinum_II = pd.read_csv('Dataset/lose/Platinum_II_lose.csv')\n",
    "win_Platinum_III = pd.read_csv('Dataset/win/Platinum_III.csv')\n",
    "lose_Platinum_III = pd.read_csv('Dataset/lose/Platinum_III_lose.csv')\n",
    "win_Platinum_IV = pd.read_csv('Dataset/win/Platinum_IV.csv')\n",
    "lose_Platinum_IV = pd.read_csv('Dataset/lose/Platinum_IV_lose.csv')\n",
    "Platinum = pd.concat([win_Platinum_I, win_Platinum_II, win_Platinum_III, win_Platinum_IV, lose_Platinum_I, lose_Platinum_II, lose_Platinum_III, lose_Platinum_IV], ignore_index=True)\n",
    "print('Platinum : ', Platinum['result'].value_counts())\n",
    "\n",
    "win_Gold_I = pd.read_csv('Dataset/win/Gold_I.csv')\n",
    "lose_Gold_I = pd.read_csv('Dataset/lose/Gold_I_lose.csv')\n",
    "win_Gold_II = pd.read_csv('Dataset/win/Gold_II.csv')\n",
    "lose_Gold_II = pd.read_csv('Dataset/lose/Gold_II_lose.csv')\n",
    "win_Gold_III= pd.read_csv('Dataset/win/Gold_III.csv')\n",
    "lose_Gold_III = pd.read_csv('Dataset/lose/Gold_III_lose.csv')\n",
    "win_Gold_IV = pd.read_csv('Dataset/win/Gold_IV.csv')\n",
    "lose_Gold_IV = pd.read_csv('Dataset/lose/Gold_IV_lose.csv')\n",
    "Gold = pd.concat([win_Gold_I, win_Gold_II, win_Gold_III, win_Gold_IV, lose_Gold_I, lose_Gold_II, lose_Gold_III, lose_Gold_IV], ignore_index=True)\n",
    "print('Gold : ', Gold['result'].value_counts())\n",
    "\n",
    "\n",
    "win_Silver_I = pd.read_csv('Dataset/win/Silver_I.csv')\n",
    "lose_Silver_I = pd.read_csv('Dataset/lose/Silver_I_lose.csv')\n",
    "win_Silver_II = pd.read_csv('Dataset/win/Silver_II.csv')\n",
    "lose_Silver_II = pd.read_csv('Dataset/lose/Silver_II_lose.csv')\n",
    "win_Silver_III = pd.read_csv('Dataset/win/Silver_III.csv')\n",
    "lose_Silver_III = pd.read_csv('Dataset/lose/Silver_III_lose.csv')\n",
    "win_Silver_IV = pd.read_csv('Dataset/win/Silver_IV.csv')\n",
    "lose_Silver_IV = pd.read_csv('Dataset/lose/Silver_IV_lose.csv')\n",
    "Silver = pd.concat([win_Silver_I, win_Silver_II, win_Silver_III, win_Silver_IV, lose_Silver_I, lose_Silver_II, lose_Silver_III, lose_Silver_IV], ignore_index=True)\n",
    "print('Silver : ', Silver['result'].value_counts())\n",
    "\n",
    "win_Bronze_I = pd.read_csv('Dataset/win/Bronze_I.csv')\n",
    "lose_Bronze_I = pd.read_csv('Dataset/lose/Bronze_I_lose.csv')\n",
    "win_Bronze_II = pd.read_csv('Dataset/win/Bronze_II.csv')\n",
    "lose_Bronze_II = pd.read_csv('Dataset/lose/Bronze_II_lose.csv')\n",
    "win_Bronze_III = pd.read_csv('Dataset/win/Bronze_III.csv')\n",
    "lose_Bronze_III = pd.read_csv('Dataset/lose/Bronze_III_lose.csv')\n",
    "win_Bronze_IV = pd.read_csv('Dataset/win/Bronze_IV.csv')\n",
    "lose_Bronze_IV = pd.read_csv('Dataset/lose/Bronze_IV_lose.csv')\n",
    "Bronze = pd.concat([win_Bronze_I, win_Bronze_II, win_Bronze_III, win_Bronze_IV, lose_Bronze_I, lose_Bronze_II, lose_Bronze_III, lose_Bronze_IV], ignore_index=True)\n",
    "print('Bronze : ', Bronze['result'].value_counts())\n",
    "\n",
    "win_Iron_I = pd.read_csv('Dataset/win/Iron_I.csv')\n",
    "lose_Iron_I = pd.read_csv('Dataset/lose/Iron_I_lose.csv')\n",
    "win_Iron_II = pd.read_csv('Dataset/win/Iron_II.csv')\n",
    "lose_Iron_II = pd.read_csv('Dataset/lose/Iron_II_lose.csv')\n",
    "win_Iron_III = pd.read_csv('Dataset/win/Iron_III.csv')\n",
    "lose_Iron_III = pd.read_csv('Dataset/lose/Iron_III_lose.csv')\n",
    "win_Iron_IV = pd.read_csv('Dataset/win/Iron_IV.csv')\n",
    "lose_Iron_IV = pd.read_csv('Dataset/lose/Iron_IV_lose.csv')\n",
    "Iron = pd.concat([win_Iron_I, win_Iron_II, win_Iron_III, win_Iron_IV, lose_Iron_I, lose_Iron_II, lose_Iron_III, lose_Iron_IV], ignore_index=True)\n",
    "print('Iron : ', Iron['result'].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c76b440c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sklearn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=200, max_depth=20, random_state = 10)\n",
    "lgbm = LGBMClassifier(n_estimators=100, verbosity=0)\n",
    "cat = CatBoostClassifier(iterations=2, depth=2, learning_rate=1)\n",
    "et = ExtraTreesClassifier(n_estimators=100, random_state = 10)\n",
    "\n",
    "xgb = XGBClassifier(n_estimators=100, learning_rate=0.2, max_depth=4, random_state = 32)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "195f4caf",
   "metadata": {},
   "source": [
    "# Original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7ad12b6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 다른 스케일링 방법과 정확도, confusion matrix를 비교하기 위한 원본\n",
    "def def_original(data, showGraph):\n",
    "    X = data.iloc[:, :20]\n",
    "    y = data.iloc[:, 20:]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 42) # 학습데이터와 평가데이터의 비율을 8:2 로 분할|\n",
    "\n",
    "    #X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=3)\n",
    "    rf.fit(X_train, y_train)\n",
    "    xgb_pre = rf.predict(X_test)\n",
    "    print('원본 RandomForest 정확도 :', round(accuracy_score(y_test, xgb_pre)*100, 2))\n",
    "    xgb_tn, xgb_fp, xgb_fn, xgb_tp = confusion_matrix(y_test, xgb_pre).ravel()\n",
    "    print(f'TN:{xgb_tn} FP:{xgb_fp} FN:{xgb_fn} TP:{xgb_tp}')\n",
    "    \n",
    "    lgbm.fit(X_train, y_train)\n",
    "    dtc_pre = lgbm.predict(X_test)\n",
    "    print('원본 LGBM 정확도 :', round(accuracy_score(y_test, dtc_pre)*100, 2))\n",
    "    dtc_tn, dtc_fp, dtc_fn, dtc_tp = confusion_matrix(y_test, dtc_pre).ravel()\n",
    "    print(f'TN:{dtc_tn} FP:{dtc_fp} FN:{dtc_fn} TP:{dtc_tp}')\n",
    "    \n",
    "    cat.fit(X_train, y_train)\n",
    "    lr_pre = cat.predict(X_test)\n",
    "    print('원본 catBoost 정확도 :', round(accuracy_score(y_test, lr_pre)*100, 2))\n",
    "    lr_tn, lr_fp, lr_fn, lr_tp = confusion_matrix(y_test, lr_pre).ravel()\n",
    "    print(f'TN:{lr_tn} FP:{lr_fp} FN:{lr_fn} TP:{lr_tp}')\n",
    "\n",
    "    et.fit(X_train, y_train)\n",
    "    et_pre = et.predict(X_test)\n",
    "    print('원본 ExtraTree 정확도 :', round(accuracy_score(y_test, et_pre)*100, 2))\n",
    "    et_tn, et_fp, et_fn, et_tp = confusion_matrix(y_test, et_pre).ravel()\n",
    "    print(f'TN:{et_tn} FP:{et_fp} FN:{et_fn} TP:{et_tp}')\n",
    "    \n",
    "    if showGraph == True:\n",
    "        cm = confusion_matrix(y_test, xgb_pre)\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "        plt.xlabel('Predicted')\n",
    "        plt.ylabel('True')\n",
    "        plt.show()\n",
    "#         np_X_train = X_train.values\n",
    "#         X_train_data = np_X_train.reshape((X_train.shape[1]*X_train.shape[0]), 1)\n",
    "#         plt.hist(X_train_data, bins=30, color= 'red', alpha = 0.7)\n",
    "#         plt.title('before data scaling')\n",
    "#         plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed578851",
   "metadata": {},
   "source": [
    "# StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9c25c520",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 평균이 0, 분산이 1인 정규분포를 갖도록 만들어준다(표준화).\n",
    "# 이상치가 존재한다면 스케일링 방법으로 적절하지 않음.\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "def def_StandardScaler(data, showGraph):\n",
    "    X = data.iloc[:, :20]\n",
    "    y = data.iloc[:, 20:]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 42) # 학습데이터와 평가데이터의 비율을 8:2 로 분할|\n",
    "    \n",
    "#     X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=3)\n",
    "    std = StandardScaler()\n",
    "    # train data는 fit 메서드를 적용시킨 후 transform, test data는 transform\n",
    "    std.fit(X_train)\n",
    "    std_X_train_scaled = std.transform(X_train)\n",
    "    std_X_test_scaled = std.transform(X_test)\n",
    "    \n",
    "    X_train = std_X_train_scaled\n",
    "    X_test = std_X_test_scaled\n",
    "\n",
    "    rf.fit(X_train, y_train)\n",
    "    xgb_pre = rf.predict(X_test)\n",
    "    print('원본 RandomForest 정확도 :', round(accuracy_score(y_test, xgb_pre)*100, 2))\n",
    "    xgb_tn, xgb_fp, xgb_fn, xgb_tp = confusion_matrix(y_test, xgb_pre).ravel()\n",
    "    print(f'TN:{xgb_tn} FP:{xgb_fp} FN:{xgb_fn} TP:{xgb_tp}')\n",
    "    \n",
    "    lgbm.fit(X_train, y_train)\n",
    "    dtc_pre = lgbm.predict(X_test)\n",
    "    print('원본 LGBM 정확도 :', round(accuracy_score(y_test, dtc_pre)*100, 2))\n",
    "    dtc_tn, dtc_fp, dtc_fn, dtc_tp = confusion_matrix(y_test, dtc_pre).ravel()\n",
    "    print(f'TN:{dtc_tn} FP:{dtc_fp} FN:{dtc_fn} TP:{dtc_tp}')\n",
    "    \n",
    "    cat.fit(X_train, y_train)\n",
    "    lr_pre = cat.predict(X_test)\n",
    "    print('원본 catBoost 정확도 :', round(accuracy_score(y_test, lr_pre)*100, 2))\n",
    "    lr_tn, lr_fp, lr_fn, lr_tp = confusion_matrix(y_test, lr_pre).ravel()\n",
    "    print(f'TN:{lr_tn} FP:{lr_fp} FN:{lr_fn} TP:{lr_tp}')\n",
    "\n",
    "    et.fit(X_train, y_train)\n",
    "    et_pre = et.predict(X_test)\n",
    "    print('원본 ExtraTree 정확도 :', round(accuracy_score(y_test, et_pre)*100, 2))\n",
    "    et_tn, et_fp, et_fn, et_tp = confusion_matrix(y_test, et_pre).ravel()\n",
    "    print(f'TN:{et_tn} FP:{et_fp} FN:{et_fn} TP:{et_tp}')\n",
    "    \n",
    "    if showGraph == True:\n",
    "        cm = confusion_matrix(y_test, xgb_pre)\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "        plt.xlabel('Predicted')\n",
    "        plt.ylabel('True')\n",
    "        plt.show()\n",
    "#         X_train_scaled_ss = std_X_train_scaled.reshape((X_train.shape[1]*X_train.shape[0]),1)\n",
    "#         plt.hist(X_train_scaled_ss, bins=30, alpha = 0.7, density = True)\n",
    "#         plt.title('StandardScaler')\n",
    "#         plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5e65181",
   "metadata": {},
   "source": [
    "# MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c5f57ce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모든 피처들이 0과 1사이의 데이터값을 갖도록 함. 최솟값 0, 최댓값 1\n",
    "# 이상치가 존재한다면 스케일링 방법으로 적절하지 않음.\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "def def_MinMaxScaler(data, showGraph):\n",
    "    X = data.iloc[:, :20]\n",
    "    y = data.iloc[:, 20:]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 42) # 학습데이터와 평가데이터의 비율을 8:2 로 분할|\n",
    "    \n",
    "#     X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=3)\n",
    "    mms = MinMaxScaler()\n",
    "    mms.fit(X_train)\n",
    "    mms_X_train_scaled = mms.transform(X_train)\n",
    "    mms_X_test_scaled = mms.transform(X_test)\n",
    "\n",
    "    X_train = mms_X_train_scaled\n",
    "    X_test = mms_X_test_scaled\n",
    "    \n",
    "    rf.fit(X_train, y_train)\n",
    "    xgb_pre = rf.predict(X_test)\n",
    "    print('원본 RandomForest 정확도 :', round(accuracy_score(y_test, xgb_pre)*100, 2))\n",
    "    xgb_tn, xgb_fp, xgb_fn, xgb_tp = confusion_matrix(y_test, xgb_pre).ravel()\n",
    "    print(f'TN:{xgb_tn} FP:{xgb_fp} FN:{xgb_fn} TP:{xgb_tp}')\n",
    "    \n",
    "    lgbm.fit(X_train, y_train)\n",
    "    dtc_pre = lgbm.predict(X_test)\n",
    "    print('원본 LGBM 정확도 :', round(accuracy_score(y_test, dtc_pre)*100, 2))\n",
    "    dtc_tn, dtc_fp, dtc_fn, dtc_tp = confusion_matrix(y_test, dtc_pre).ravel()\n",
    "    print(f'TN:{dtc_tn} FP:{dtc_fp} FN:{dtc_fn} TP:{dtc_tp}')\n",
    "    \n",
    "    cat.fit(X_train, y_train)\n",
    "    lr_pre = cat.predict(X_test)\n",
    "    print('원본 catBoost 정확도 :', round(accuracy_score(y_test, lr_pre)*100, 2))\n",
    "    lr_tn, lr_fp, lr_fn, lr_tp = confusion_matrix(y_test, lr_pre).ravel()\n",
    "    print(f'TN:{lr_tn} FP:{lr_fp} FN:{lr_fn} TP:{lr_tp}')\n",
    "\n",
    "    et.fit(X_train, y_train)\n",
    "    et_pre = et.predict(X_test)\n",
    "    print('원본 ExtraTree 정확도 :', round(accuracy_score(y_test, et_pre)*100, 2))\n",
    "    et_tn, et_fp, et_fn, et_tp = confusion_matrix(y_test, et_pre).ravel()\n",
    "    print(f'TN:{et_tn} FP:{et_fp} FN:{et_fn} TP:{et_tp}')\n",
    "    if showGraph == True:\n",
    "        cm = confusion_matrix(y_test, xgb_pre)\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "        plt.xlabel('Predicted')\n",
    "        plt.ylabel('True')\n",
    "        plt.show()\n",
    "#         mms_X_train_scaled_reshape = mms_X_train_scaled.reshape((X_train.shape[1]*X_train.shape[0]),1)\n",
    "#         plt.hist(mms_X_train_scaled_reshape, bins=30, color='green', alpha = 0.7)\n",
    "#         plt.title('MinMaxScaler')\n",
    "#         plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16c99d46",
   "metadata": {},
   "source": [
    "# MaxAbsScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "eca0db95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모든 피처들의 절댓값이 0과 1 사이\n",
    "# 데이터가 -1과 1사이의 범위에 존재\n",
    "# 이상치가 존재한다면 스케일링 방법으로 적절하지 않음.\n",
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "def def_MaxAbsScaler(data, showGraph):\n",
    "    X = data.iloc[:, :20]\n",
    "    y = data.iloc[:, 20:]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 42) # 학습데이터와 평가데이터의 비율을 8:2 로 분할|\n",
    "#     X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=3)\n",
    "    mas = MaxAbsScaler()\n",
    "    mas.fit(X_train)\n",
    "    mas_X_train_scaled = mas.transform(X_train)\n",
    "    mas_X_test_scaled = mas.transform(X_test)\n",
    "    \n",
    "    X_train = mas_X_train_scaled\n",
    "    X_test = mas_X_test_scaled\n",
    "\n",
    "    rf.fit(X_train, y_train)\n",
    "    xgb_pre = rf.predict(X_test)\n",
    "    print('원본 RandomForest 정확도 :', round(accuracy_score(y_test, xgb_pre)*100, 2))\n",
    "    xgb_tn, xgb_fp, xgb_fn, xgb_tp = confusion_matrix(y_test, xgb_pre).ravel()\n",
    "    print(f'TN:{xgb_tn} FP:{xgb_fp} FN:{xgb_fn} TP:{xgb_tp}')\n",
    "    \n",
    "    lgbm.fit(X_train, y_train)\n",
    "    dtc_pre = lgbm.predict(X_test)\n",
    "    print('원본 LGBM 정확도 :', round(accuracy_score(y_test, dtc_pre)*100, 2))\n",
    "    dtc_tn, dtc_fp, dtc_fn, dtc_tp = confusion_matrix(y_test, dtc_pre).ravel()\n",
    "    print(f'TN:{dtc_tn} FP:{dtc_fp} FN:{dtc_fn} TP:{dtc_tp}')\n",
    "    \n",
    "    cat.fit(X_train, y_train)\n",
    "    lr_pre = cat.predict(X_test)\n",
    "    print('원본 catBoost 정확도 :', round(accuracy_score(y_test, lr_pre)*100, 2))\n",
    "    lr_tn, lr_fp, lr_fn, lr_tp = confusion_matrix(y_test, lr_pre).ravel()\n",
    "    print(f'TN:{lr_tn} FP:{lr_fp} FN:{lr_fn} TP:{lr_tp}')\n",
    "\n",
    "    et.fit(X_train, y_train)\n",
    "    et_pre = et.predict(X_test)\n",
    "    print('원본 ExtraTree 정확도 :', round(accuracy_score(y_test, et_pre)*100, 2))\n",
    "    et_tn, et_fp, et_fn, et_tp = confusion_matrix(y_test, et_pre).ravel()\n",
    "    print(f'TN:{et_tn} FP:{et_fp} FN:{et_fn} TP:{et_tp}')\n",
    "    if showGraph == True:\n",
    "        cm = confusion_matrix(y_test, xgb_pre)\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "        plt.xlabel('Predicted')\n",
    "        plt.ylabel('True')\n",
    "        plt.show()\n",
    "#         mas_X_train_scaled_reshape = mas_X_train_scaled.reshape((X_train.shape[1]*X_train.shape[0]),1)\n",
    "#         plt.hist(mas_X_train_scaled_reshape, bins=30, color='yellow', alpha = 0.7)\n",
    "#         plt.title('MaxAbsScaler')\n",
    "#         plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30aca5b7",
   "metadata": {},
   "source": [
    "# RobustScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c567e053",
   "metadata": {},
   "outputs": [],
   "source": [
    "# StandardScaler는 평균과 분산을 사용했지만, RobustScaler는 중간값(median)과 사분위값(quartile)을 사용\n",
    "# 따라서 이상치의 영향을 최소화할 수 있음.\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "def def_RobustScaler(data, showGraph):\n",
    "    X = data.iloc[:, :20]\n",
    "    y = data.iloc[:, 20:]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 42) # 학습데이터와 평가데이터의 비율을 8:2 로 분할|\n",
    "#     X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=3)\n",
    "    rbs = RobustScaler()\n",
    "    rbs_X_train_scaled = rbs.fit_transform(X_train)\n",
    "    rbs_X_test_scaled = rbs.transform(X_test)\n",
    "\n",
    "    X_train = rbs_X_train_scaled\n",
    "    X_test = rbs_X_test_scaled\n",
    "    \n",
    "    rf.fit(X_train, y_train)\n",
    "    xgb_pre = rf.predict(X_test)\n",
    "    print('원본 RandomForest 정확도 :', round(accuracy_score(y_test, xgb_pre)*100, 2))\n",
    "    xgb_tn, xgb_fp, xgb_fn, xgb_tp = confusion_matrix(y_test, xgb_pre).ravel()\n",
    "    print(f'TN:{xgb_tn} FP:{xgb_fp} FN:{xgb_fn} TP:{xgb_tp}')\n",
    "    \n",
    "    lgbm.fit(X_train, y_train)\n",
    "    dtc_pre = lgbm.predict(X_test)\n",
    "    print('원본 LGBM 정확도 :', round(accuracy_score(y_test, dtc_pre)*100, 2))\n",
    "    dtc_tn, dtc_fp, dtc_fn, dtc_tp = confusion_matrix(y_test, dtc_pre).ravel()\n",
    "    print(f'TN:{dtc_tn} FP:{dtc_fp} FN:{dtc_fn} TP:{dtc_tp}')\n",
    "    \n",
    "    cat.fit(X_train, y_train)\n",
    "    lr_pre = cat.predict(X_test)\n",
    "    print('원본 catBoost 정확도 :', round(accuracy_score(y_test, lr_pre)*100, 2))\n",
    "    lr_tn, lr_fp, lr_fn, lr_tp = confusion_matrix(y_test, lr_pre).ravel()\n",
    "    print(f'TN:{lr_tn} FP:{lr_fp} FN:{lr_fn} TP:{lr_tp}')\n",
    "\n",
    "    et.fit(X_train, y_train)\n",
    "    et_pre = et.predict(X_test)\n",
    "    print('원본 ExtraTree 정확도 :', round(accuracy_score(y_test, et_pre)*100, 2))\n",
    "    et_tn, et_fp, et_fn, et_tp = confusion_matrix(y_test, et_pre).ravel()\n",
    "    print(f'TN:{et_tn} FP:{et_fp} FN:{et_fn} TP:{et_tp}')\n",
    "    if showGraph == True:\n",
    "        cm = confusion_matrix(y_test, xgb_pre)\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "        plt.xlabel('Predicted')\n",
    "        plt.ylabel('True')\n",
    "        plt.show()\n",
    "#         rbs_X_train_scaled_reshape = rbs_X_train_scaled.reshape((X_train.shape[1]*X_train.shape[0]),1)\n",
    "#         plt.hist(rbs_X_train_scaled_reshape, bins=30, color='pink', alpha = 0.7)\n",
    "#         plt.title('RobustScaler')\n",
    "#         plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f4e64e4",
   "metadata": {},
   "source": [
    "# Normalizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ed318525",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 앞의 4가지 방법은 열을 대상으로 진행, Normalizer는 각 행(row)마다 정규화\n",
    "# 한 행의 모든 피처들 사이의 유클리드 거리가 1이 되도록 데이터값을 만들어준다.\n",
    "from sklearn.preprocessing import Normalizer\n",
    "def def_Normalizer(data, showGraph):\n",
    "    X = data.iloc[:, :20]\n",
    "    y = data.iloc[:, 20:]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 42) # 학습데이터와 평가데이터의 비율을 8:2 로 분할|\n",
    "#     X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=3)\n",
    "    norm = Normalizer()\n",
    "    norm_X_train_scaled = norm.fit_transform(X_train)\n",
    "    norm_X_test_scaled = norm.transform(X_test)\n",
    "    \n",
    "    X_train = norm_X_train_scaled\n",
    "    X_test = norm_X_test_scaled\n",
    "\n",
    "    rf.fit(X_train, y_train)\n",
    "    xgb_pre = rf.predict(X_test)\n",
    "    print('원본 RandomForest 정확도 :', round(accuracy_score(y_test, xgb_pre)*100, 2))\n",
    "    xgb_tn, xgb_fp, xgb_fn, xgb_tp = confusion_matrix(y_test, xgb_pre).ravel()\n",
    "    print(f'TN:{xgb_tn} FP:{xgb_fp} FN:{xgb_fn} TP:{xgb_tp}')\n",
    "    \n",
    "    lgbm.fit(X_train, y_train)\n",
    "    dtc_pre = lgbm.predict(X_test)\n",
    "    print('원본 LGBM 정확도 :', round(accuracy_score(y_test, dtc_pre)*100, 2))\n",
    "    dtc_tn, dtc_fp, dtc_fn, dtc_tp = confusion_matrix(y_test, dtc_pre).ravel()\n",
    "    print(f'TN:{dtc_tn} FP:{dtc_fp} FN:{dtc_fn} TP:{dtc_tp}')\n",
    "    \n",
    "    cat.fit(X_train, y_train)\n",
    "    lr_pre = cat.predict(X_test)\n",
    "    print('원본 catBoost 정확도 :', round(accuracy_score(y_test, lr_pre)*100, 2))\n",
    "    lr_tn, lr_fp, lr_fn, lr_tp = confusion_matrix(y_test, lr_pre).ravel()\n",
    "    print(f'TN:{lr_tn} FP:{lr_fp} FN:{lr_fn} TP:{lr_tp}')\n",
    "\n",
    "    et.fit(X_train, y_train)\n",
    "    et_pre = et.predict(X_test)\n",
    "    print('원본 ExtraTree 정확도 :', round(accuracy_score(y_test, et_pre)*100, 2))\n",
    "    et_tn, et_fp, et_fn, et_tp = confusion_matrix(y_test, et_pre).ravel()\n",
    "    print(f'TN:{et_tn} FP:{et_fp} FN:{et_fn} TP:{et_tp}')\n",
    "    \n",
    "    if showGraph == True:\n",
    "        cm = confusion_matrix(y_test, xgb_pre)\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "        plt.xlabel('Predicted')\n",
    "        plt.ylabel('True')\n",
    "        plt.show()\n",
    "#         norm_X_train_scaled_reshape = norm_X_train_scaled.reshape((X_train.shape[1]*X_train.shape[0]),1)\n",
    "#         plt.hist(norm_X_train_scaled_reshape, bins=30, color='orange', alpha = 0.7)\n",
    "#         plt.title('Normalizer')\n",
    "#         plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c0ef1efe",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py:1152: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "원본 RandomForest 정확도 : 76.9\n",
      "TN:4368 FP:1347 FN:1294 TP:4426\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:97: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:132: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, dtype=self.classes_.dtype, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "원본 LGBM 정확도 : 76.69\n",
      "TN:4375 FP:1340 FN:1325 TP:4395\n",
      "0:\tlearn: 0.5417741\ttotal: 6.07ms\tremaining: 6.07ms\n",
      "1:\tlearn: 0.5213310\ttotal: 11.1ms\tremaining: 0us\n",
      "원본 catBoost 정확도 : 75.54\n",
      "TN:4276 FP:1439 FN:1358 TP:4362\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py:1152: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "원본 ExtraTree 정확도 : 76.48\n",
      "TN:4384 FP:1331 FN:1359 TP:4361\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py:1152: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "원본 RandomForest 정확도 : 76.93\n",
      "TN:4373 FP:1342 FN:1296 TP:4424\n",
      "원본 LGBM 정확도 : 76.89\n",
      "TN:4381 FP:1334 FN:1309 TP:4411\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:97: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:132: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, dtype=self.classes_.dtype, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 0.5417741\ttotal: 4.25ms\tremaining: 4.25ms\n",
      "1:\tlearn: 0.5213310\ttotal: 8.81ms\tremaining: 0us\n",
      "원본 catBoost 정확도 : 75.54\n",
      "TN:4276 FP:1439 FN:1358 TP:4362\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py:1152: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "원본 ExtraTree 정확도 : 76.48\n",
      "TN:4384 FP:1331 FN:1359 TP:4361\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py:1152: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "원본 RandomForest 정확도 : 76.92\n",
      "TN:4368 FP:1347 FN:1292 TP:4428\n",
      "원본 LGBM 정확도 : 76.73\n",
      "TN:4363 FP:1352 FN:1309 TP:4411\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:97: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:132: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, dtype=self.classes_.dtype, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 0.5417741\ttotal: 7.21ms\tremaining: 7.21ms\n",
      "1:\tlearn: 0.5213310\ttotal: 13.6ms\tremaining: 0us\n",
      "원본 catBoost 정확도 : 75.54\n",
      "TN:4276 FP:1439 FN:1358 TP:4362\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py:1152: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\User\\OneDrive - 인하대학교\\권우영\\인하대학교\\2학년\\강의\\(2-2)데이터사이언스\\TeamProject\\코드\\Scaling.ipynb Cell 16\u001b[0m line \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/User/OneDrive%20-%20%EC%9D%B8%ED%95%98%EB%8C%80%ED%95%99%EA%B5%90/%EA%B6%8C%EC%9A%B0%EC%98%81/%EC%9D%B8%ED%95%98%EB%8C%80%ED%95%99%EA%B5%90/2%ED%95%99%EB%85%84/%EA%B0%95%EC%9D%98/%282-2%29%EB%8D%B0%EC%9D%B4%ED%84%B0%EC%82%AC%EC%9D%B4%EC%96%B8%EC%8A%A4/TeamProject/%EC%BD%94%EB%93%9C/Scaling.ipynb#X44sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m def_original(data, showGraph)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/User/OneDrive%20-%20%EC%9D%B8%ED%95%98%EB%8C%80%ED%95%99%EA%B5%90/%EA%B6%8C%EC%9A%B0%EC%98%81/%EC%9D%B8%ED%95%98%EB%8C%80%ED%95%99%EA%B5%90/2%ED%95%99%EB%85%84/%EA%B0%95%EC%9D%98/%282-2%29%EB%8D%B0%EC%9D%B4%ED%84%B0%EC%82%AC%EC%9D%B4%EC%96%B8%EC%8A%A4/TeamProject/%EC%BD%94%EB%93%9C/Scaling.ipynb#X44sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m def_StandardScaler(data, showGraph)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/User/OneDrive%20-%20%EC%9D%B8%ED%95%98%EB%8C%80%ED%95%99%EA%B5%90/%EA%B6%8C%EC%9A%B0%EC%98%81/%EC%9D%B8%ED%95%98%EB%8C%80%ED%95%99%EA%B5%90/2%ED%95%99%EB%85%84/%EA%B0%95%EC%9D%98/%282-2%29%EB%8D%B0%EC%9D%B4%ED%84%B0%EC%82%AC%EC%9D%B4%EC%96%B8%EC%8A%A4/TeamProject/%EC%BD%94%EB%93%9C/Scaling.ipynb#X44sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m def_MinMaxScaler(data, showGraph)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/User/OneDrive%20-%20%EC%9D%B8%ED%95%98%EB%8C%80%ED%95%99%EA%B5%90/%EA%B6%8C%EC%9A%B0%EC%98%81/%EC%9D%B8%ED%95%98%EB%8C%80%ED%95%99%EA%B5%90/2%ED%95%99%EB%85%84/%EA%B0%95%EC%9D%98/%282-2%29%EB%8D%B0%EC%9D%B4%ED%84%B0%EC%82%AC%EC%9D%B4%EC%96%B8%EC%8A%A4/TeamProject/%EC%BD%94%EB%93%9C/Scaling.ipynb#X44sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m def_MaxAbsScaler(data, showGraph)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/User/OneDrive%20-%20%EC%9D%B8%ED%95%98%EB%8C%80%ED%95%99%EA%B5%90/%EA%B6%8C%EC%9A%B0%EC%98%81/%EC%9D%B8%ED%95%98%EB%8C%80%ED%95%99%EA%B5%90/2%ED%95%99%EB%85%84/%EA%B0%95%EC%9D%98/%282-2%29%EB%8D%B0%EC%9D%B4%ED%84%B0%EC%82%AC%EC%9D%B4%EC%96%B8%EC%8A%A4/TeamProject/%EC%BD%94%EB%93%9C/Scaling.ipynb#X44sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m def_RobustScaler(data, showGraph)\n",
      "\u001b[1;32mc:\\Users\\User\\OneDrive - 인하대학교\\권우영\\인하대학교\\2학년\\강의\\(2-2)데이터사이언스\\TeamProject\\코드\\Scaling.ipynb Cell 16\u001b[0m line \u001b[0;36m3\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/User/OneDrive%20-%20%EC%9D%B8%ED%95%98%EB%8C%80%ED%95%99%EA%B5%90/%EA%B6%8C%EC%9A%B0%EC%98%81/%EC%9D%B8%ED%95%98%EB%8C%80%ED%95%99%EA%B5%90/2%ED%95%99%EB%85%84/%EA%B0%95%EC%9D%98/%282-2%29%EB%8D%B0%EC%9D%B4%ED%84%B0%EC%82%AC%EC%9D%B4%EC%96%B8%EC%8A%A4/TeamProject/%EC%BD%94%EB%93%9C/Scaling.ipynb#X44sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m lr_tn, lr_fp, lr_fn, lr_tp \u001b[39m=\u001b[39m confusion_matrix(y_test, lr_pre)\u001b[39m.\u001b[39mravel()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/User/OneDrive%20-%20%EC%9D%B8%ED%95%98%EB%8C%80%ED%95%99%EA%B5%90/%EA%B6%8C%EC%9A%B0%EC%98%81/%EC%9D%B8%ED%95%98%EB%8C%80%ED%95%99%EA%B5%90/2%ED%95%99%EB%85%84/%EA%B0%95%EC%9D%98/%282-2%29%EB%8D%B0%EC%9D%B4%ED%84%B0%EC%82%AC%EC%9D%B4%EC%96%B8%EC%8A%A4/TeamProject/%EC%BD%94%EB%93%9C/Scaling.ipynb#X44sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mTN:\u001b[39m\u001b[39m{\u001b[39;00mlr_tn\u001b[39m}\u001b[39;00m\u001b[39m FP:\u001b[39m\u001b[39m{\u001b[39;00mlr_fp\u001b[39m}\u001b[39;00m\u001b[39m FN:\u001b[39m\u001b[39m{\u001b[39;00mlr_fn\u001b[39m}\u001b[39;00m\u001b[39m TP:\u001b[39m\u001b[39m{\u001b[39;00mlr_tp\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/User/OneDrive%20-%20%EC%9D%B8%ED%95%98%EB%8C%80%ED%95%99%EA%B5%90/%EA%B6%8C%EC%9A%B0%EC%98%81/%EC%9D%B8%ED%95%98%EB%8C%80%ED%95%99%EA%B5%90/2%ED%95%99%EB%85%84/%EA%B0%95%EC%9D%98/%282-2%29%EB%8D%B0%EC%9D%B4%ED%84%B0%EC%82%AC%EC%9D%B4%EC%96%B8%EC%8A%A4/TeamProject/%EC%BD%94%EB%93%9C/Scaling.ipynb#X44sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m et\u001b[39m.\u001b[39;49mfit(X_train, y_train)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/User/OneDrive%20-%20%EC%9D%B8%ED%95%98%EB%8C%80%ED%95%99%EA%B5%90/%EA%B6%8C%EC%9A%B0%EC%98%81/%EC%9D%B8%ED%95%98%EB%8C%80%ED%95%99%EA%B5%90/2%ED%95%99%EB%85%84/%EA%B0%95%EC%9D%98/%282-2%29%EB%8D%B0%EC%9D%B4%ED%84%B0%EC%82%AC%EC%9D%B4%EC%96%B8%EC%8A%A4/TeamProject/%EC%BD%94%EB%93%9C/Scaling.ipynb#X44sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m et_pre \u001b[39m=\u001b[39m et\u001b[39m.\u001b[39mpredict(X_test)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/User/OneDrive%20-%20%EC%9D%B8%ED%95%98%EB%8C%80%ED%95%99%EA%B5%90/%EA%B6%8C%EC%9A%B0%EC%98%81/%EC%9D%B8%ED%95%98%EB%8C%80%ED%95%99%EA%B5%90/2%ED%95%99%EB%85%84/%EA%B0%95%EC%9D%98/%282-2%29%EB%8D%B0%EC%9D%B4%ED%84%B0%EC%82%AC%EC%9D%B4%EC%96%B8%EC%8A%A4/TeamProject/%EC%BD%94%EB%93%9C/Scaling.ipynb#X44sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39m원본 ExtraTree 정확도 :\u001b[39m\u001b[39m'\u001b[39m, \u001b[39mround\u001b[39m(accuracy_score(y_test, et_pre)\u001b[39m*\u001b[39m\u001b[39m100\u001b[39m, \u001b[39m2\u001b[39m))\n",
      "File \u001b[1;32mc:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py:1152\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1145\u001b[0m     estimator\u001b[39m.\u001b[39m_validate_params()\n\u001b[0;32m   1147\u001b[0m \u001b[39mwith\u001b[39;00m config_context(\n\u001b[0;32m   1148\u001b[0m     skip_parameter_validation\u001b[39m=\u001b[39m(\n\u001b[0;32m   1149\u001b[0m         prefer_skip_nested_validation \u001b[39mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1150\u001b[0m     )\n\u001b[0;32m   1151\u001b[0m ):\n\u001b[1;32m-> 1152\u001b[0m     \u001b[39mreturn\u001b[39;00m fit_method(estimator, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:456\u001b[0m, in \u001b[0;36mBaseForest.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    445\u001b[0m trees \u001b[39m=\u001b[39m [\n\u001b[0;32m    446\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_make_estimator(append\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, random_state\u001b[39m=\u001b[39mrandom_state)\n\u001b[0;32m    447\u001b[0m     \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(n_more_estimators)\n\u001b[0;32m    448\u001b[0m ]\n\u001b[0;32m    450\u001b[0m \u001b[39m# Parallel loop: we prefer the threading backend as the Cython code\u001b[39;00m\n\u001b[0;32m    451\u001b[0m \u001b[39m# for fitting the trees is internally releasing the Python GIL\u001b[39;00m\n\u001b[0;32m    452\u001b[0m \u001b[39m# making threading more efficient than multiprocessing in\u001b[39;00m\n\u001b[0;32m    453\u001b[0m \u001b[39m# that case. However, for joblib 0.12+ we respect any\u001b[39;00m\n\u001b[0;32m    454\u001b[0m \u001b[39m# parallel_backend contexts set at a higher level,\u001b[39;00m\n\u001b[0;32m    455\u001b[0m \u001b[39m# since correctness does not rely on using threads.\u001b[39;00m\n\u001b[1;32m--> 456\u001b[0m trees \u001b[39m=\u001b[39m Parallel(\n\u001b[0;32m    457\u001b[0m     n_jobs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mn_jobs,\n\u001b[0;32m    458\u001b[0m     verbose\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mverbose,\n\u001b[0;32m    459\u001b[0m     prefer\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mthreads\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m    460\u001b[0m )(\n\u001b[0;32m    461\u001b[0m     delayed(_parallel_build_trees)(\n\u001b[0;32m    462\u001b[0m         t,\n\u001b[0;32m    463\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbootstrap,\n\u001b[0;32m    464\u001b[0m         X,\n\u001b[0;32m    465\u001b[0m         y,\n\u001b[0;32m    466\u001b[0m         sample_weight,\n\u001b[0;32m    467\u001b[0m         i,\n\u001b[0;32m    468\u001b[0m         \u001b[39mlen\u001b[39;49m(trees),\n\u001b[0;32m    469\u001b[0m         verbose\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mverbose,\n\u001b[0;32m    470\u001b[0m         class_weight\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mclass_weight,\n\u001b[0;32m    471\u001b[0m         n_samples_bootstrap\u001b[39m=\u001b[39;49mn_samples_bootstrap,\n\u001b[0;32m    472\u001b[0m     )\n\u001b[0;32m    473\u001b[0m     \u001b[39mfor\u001b[39;49;00m i, t \u001b[39min\u001b[39;49;00m \u001b[39menumerate\u001b[39;49m(trees)\n\u001b[0;32m    474\u001b[0m )\n\u001b[0;32m    476\u001b[0m \u001b[39m# Collect newly grown trees\u001b[39;00m\n\u001b[0;32m    477\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mestimators_\u001b[39m.\u001b[39mextend(trees)\n",
      "File \u001b[1;32mc:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\parallel.py:65\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     60\u001b[0m config \u001b[39m=\u001b[39m get_config()\n\u001b[0;32m     61\u001b[0m iterable_with_config \u001b[39m=\u001b[39m (\n\u001b[0;32m     62\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[0;32m     63\u001b[0m     \u001b[39mfor\u001b[39;00m delayed_func, args, kwargs \u001b[39min\u001b[39;00m iterable\n\u001b[0;32m     64\u001b[0m )\n\u001b[1;32m---> 65\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__call__\u001b[39;49m(iterable_with_config)\n",
      "File \u001b[1;32mc:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\joblib\\parallel.py:1863\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1861\u001b[0m     output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_sequential_output(iterable)\n\u001b[0;32m   1862\u001b[0m     \u001b[39mnext\u001b[39m(output)\n\u001b[1;32m-> 1863\u001b[0m     \u001b[39mreturn\u001b[39;00m output \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreturn_generator \u001b[39melse\u001b[39;00m \u001b[39mlist\u001b[39;49m(output)\n\u001b[0;32m   1865\u001b[0m \u001b[39m# Let's create an ID that uniquely identifies the current call. If the\u001b[39;00m\n\u001b[0;32m   1866\u001b[0m \u001b[39m# call is interrupted early and that the same instance is immediately\u001b[39;00m\n\u001b[0;32m   1867\u001b[0m \u001b[39m# re-used, this id will be used to prevent workers that were\u001b[39;00m\n\u001b[0;32m   1868\u001b[0m \u001b[39m# concurrently finalizing a task from the previous call to run the\u001b[39;00m\n\u001b[0;32m   1869\u001b[0m \u001b[39m# callback.\u001b[39;00m\n\u001b[0;32m   1870\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n",
      "File \u001b[1;32mc:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\joblib\\parallel.py:1792\u001b[0m, in \u001b[0;36mParallel._get_sequential_output\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1790\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_dispatched_batches \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m   1791\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_dispatched_tasks \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m-> 1792\u001b[0m res \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1793\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_completed_tasks \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m   1794\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprint_progress()\n",
      "File \u001b[1;32mc:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\parallel.py:127\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    125\u001b[0m     config \u001b[39m=\u001b[39m {}\n\u001b[0;32m    126\u001b[0m \u001b[39mwith\u001b[39;00m config_context(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mconfig):\n\u001b[1;32m--> 127\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfunction(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:190\u001b[0m, in \u001b[0;36m_parallel_build_trees\u001b[1;34m(tree, bootstrap, X, y, sample_weight, tree_idx, n_trees, verbose, class_weight, n_samples_bootstrap)\u001b[0m\n\u001b[0;32m    188\u001b[0m     tree\u001b[39m.\u001b[39mfit(X, y, sample_weight\u001b[39m=\u001b[39mcurr_sample_weight, check_input\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m    189\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 190\u001b[0m     tree\u001b[39m.\u001b[39;49mfit(X, y, sample_weight\u001b[39m=\u001b[39;49msample_weight, check_input\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[0;32m    192\u001b[0m \u001b[39mreturn\u001b[39;00m tree\n",
      "File \u001b[1;32mc:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py:1152\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1145\u001b[0m     estimator\u001b[39m.\u001b[39m_validate_params()\n\u001b[0;32m   1147\u001b[0m \u001b[39mwith\u001b[39;00m config_context(\n\u001b[0;32m   1148\u001b[0m     skip_parameter_validation\u001b[39m=\u001b[39m(\n\u001b[0;32m   1149\u001b[0m         prefer_skip_nested_validation \u001b[39mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1150\u001b[0m     )\n\u001b[0;32m   1151\u001b[0m ):\n\u001b[1;32m-> 1152\u001b[0m     \u001b[39mreturn\u001b[39;00m fit_method(estimator, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\tree\\_classes.py:959\u001b[0m, in \u001b[0;36mDecisionTreeClassifier.fit\u001b[1;34m(self, X, y, sample_weight, check_input)\u001b[0m\n\u001b[0;32m    928\u001b[0m \u001b[39m@_fit_context\u001b[39m(prefer_skip_nested_validation\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m    929\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfit\u001b[39m(\u001b[39mself\u001b[39m, X, y, sample_weight\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, check_input\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m):\n\u001b[0;32m    930\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Build a decision tree classifier from the training set (X, y).\u001b[39;00m\n\u001b[0;32m    931\u001b[0m \n\u001b[0;32m    932\u001b[0m \u001b[39m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    956\u001b[0m \u001b[39m        Fitted estimator.\u001b[39;00m\n\u001b[0;32m    957\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 959\u001b[0m     \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m_fit(\n\u001b[0;32m    960\u001b[0m         X,\n\u001b[0;32m    961\u001b[0m         y,\n\u001b[0;32m    962\u001b[0m         sample_weight\u001b[39m=\u001b[39;49msample_weight,\n\u001b[0;32m    963\u001b[0m         check_input\u001b[39m=\u001b[39;49mcheck_input,\n\u001b[0;32m    964\u001b[0m     )\n\u001b[0;32m    965\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\tree\\_classes.py:443\u001b[0m, in \u001b[0;36mBaseDecisionTree._fit\u001b[1;34m(self, X, y, sample_weight, check_input, missing_values_in_feature_mask)\u001b[0m\n\u001b[0;32m    432\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    433\u001b[0m     builder \u001b[39m=\u001b[39m BestFirstTreeBuilder(\n\u001b[0;32m    434\u001b[0m         splitter,\n\u001b[0;32m    435\u001b[0m         min_samples_split,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    440\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmin_impurity_decrease,\n\u001b[0;32m    441\u001b[0m     )\n\u001b[1;32m--> 443\u001b[0m builder\u001b[39m.\u001b[39;49mbuild(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtree_, X, y, sample_weight, missing_values_in_feature_mask)\n\u001b[0;32m    445\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_outputs_ \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m \u001b[39mand\u001b[39;00m is_classifier(\u001b[39mself\u001b[39m):\n\u001b[0;32m    446\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_classes_ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_classes_[\u001b[39m0\u001b[39m]\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# SMOTE, SMOTENC, SMOTEN, ADASYN, BorderlineSMOTE, KMeansSMOTE, SVMSMOTE \n",
    "# showGraph -> confusion matrix 출력 여부\n",
    "data, showGraph = Bronze, False\n",
    "data= data.drop(['matchId'],axis=1)\n",
    "data= data.drop(['queueId'],axis=1)\n",
    "data= data.drop(['K-WIN-top'],axis=1)\n",
    "data= data.drop(['K-LOSE-top'],axis=1)\n",
    "data= data.drop(['K-WIN-jug'],axis=1)\n",
    "data= data.drop(['K-LOSE-jug'],axis=1)\n",
    "data= data.drop(['K-WIN-mid'],axis=1)\n",
    "data= data.drop(['K-LOSE-mid'],axis=1)\n",
    "data= data.drop(['K-WIN-ad'],axis=1)\n",
    "data= data.drop(['K-LOSE-ad'],axis=1)\n",
    "data= data.drop(['K-WIN-sup'],axis=1)\n",
    "data= data.drop(['K-LOSE-sup'],axis=1)\n",
    "data= data.drop(['LOSE_controlWARDPlaced'],axis=1)\n",
    "data= data.drop(['WIN_controlWARDPlaced'],axis=1)\n",
    "data= data.drop(['Diff-A'],axis=1)\n",
    "def_original(data, showGraph)\n",
    "def_StandardScaler(data, showGraph)\n",
    "def_MinMaxScaler(data, showGraph)\n",
    "def_MaxAbsScaler(data, showGraph)\n",
    "def_RobustScaler(data, showGraph)\n",
    "def_Normalizer(data, showGraph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d3f44ad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
